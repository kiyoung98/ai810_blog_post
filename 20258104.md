---
layout: default
title: "20258104"
---

# Implicit Transfer Operators and Boltzmann Priors for Accelerating Molecular Dynamics: AI810 Blog Post (20258104)

Molecular dynamics (MD) simulations are a cornerstone for studying molecular systems, but they are notoriously slow when rare events (like protein folding or ligand unbinding) occur on long timescales. MD must use very small time-steps (~fs) for stability, yet phenomena of interest can span microseconds to seconds. Generating sufficiently long trajectories with standard MD becomes impractical. Recently, generative models have been proposed to learn the system’s dynamics, allowing much larger time-steps (ns~$\mu$s) by modeling the transition probability density of the system’s state over longer intervals as shown in Figure 1. This approach treats MD as a Markov process with state $x_t$ and transition density $p\bigl(x_{t+\Delta t}\mid x_t\bigr)$, and uses machine learning to approximate these transitions directly. In this post, we focus on two papers: **Implicit Transfer Operator (ITO) Learning** [1] and its extension **Boltzmann Priors for ITO (BoPITO)** [2]. We will cover their methodology, including the transfer-operator formalism, and how BoPITO improves ITO in data efficiency and equilibrium accuracy.

![Figure 1: Initial state $x_t$ (left) and sampled state $x_{t+\Delta t}$ (right).](https://hackmd.io/_uploads/BJAruBgzeg.png)  
###### *Figure 1. Initial state $x_t$ (left) and sampled state $x_{t+\Delta t}$ from learned transition probability distribution $p_{\theta}\bigl(x_{t+\Delta t}\mid x_t\bigr)$. (출처: [3])*

---

## Background: Markov Transfer Operators in Molecular Dynamics

**Molecular Dynamics as a Markov Process.**  
In MD, a molecular system’s time-evolution is often modeled by a stochastic differential equation (e.g. Langevin dynamics) under a potential energy function $U(x)$. When discretized with a small time-step $\tau$, this yields a Markov chain  
$$
x_0 \;\to\; x_\tau \;\to\; x_{2\tau} \;\to\; \dots
$$  
with transition density $p_\tau\bigl(x_{t+\tau}\mid x_t\bigr)$ at each step. Under ergodicity, the process has a stationary distribution $\mu(x)$, called the **Boltzmann distribution**:

$$
\mu(x) \;=\; \frac{\exp\bigl(-\beta\,U(x)\bigr)}{Z}\,, 
\quad Z \;=\; \int_{\Omega} e^{-\beta\,U(x)} \, dx, \tag{1}
$$

where $\beta$ is the inverse temperature and $Z$ is the partition function. Important physical quantities can be written as expectations under $\mu(x)$ (stationary observables) or along the path of the Markov chain (dynamic observables).

**Transfer Operator Formalism.**  
The **transfer operator** $T_\Omega(\tau)$ is a linear operator that evolves a probability density $\rho(x)$ forward by time $\tau$. Specifically,  
$$
\bigl[T_\Omega(\tau)\circ \rho\bigr](x_{t+\tau})
  \;=\; \frac{1}{\mu(x_{t+\tau})} \int_{\Omega} \rho(x_t)\;p_\tau\bigl(x_{t+\tau}\mid x_t\bigr)\;d\mu(x_t).
$$  
Over $N$ steps, this becomes:

$$
\bigl[T_\Omega(N\tau)\circ \rho\bigr](x_{t+N\tau})
  \;=\; \frac{1}{\mu(x_{t+N\tau})}\int_{\Omega} \rho(x_t)\,p_{N\tau}\bigl(x_{t+N\tau}\mid x_t\bigr)\,d\mu(x_t).
  \tag{2}
$$

The spectral decomposition of $T_\Omega$ is given by:

$$
\bigl[T_\Omega(N\tau)\circ \rho\bigr](x)
  \;=\; \sum_{i=1}^{\infty} \lambda_i(N\tau)\,\bigl\langle \varphi_i, \rho\bigr\rangle\,\psi_i(x),
  \quad \bigl\langle \varphi_i, \rho\bigr\rangle = \int_{\Omega} \varphi_i(x)\,\rho(x)\,dx.
  \tag{3}
$$

- $\lambda_1(N\tau) = 1$, with eigenfunctions $\psi_1(x)=1$ and $\varphi_1(x)=\mu(x)$.  
- All other eigenvalues satisfy $|\lambda_i|<1$ and govern slow relaxation processes (e.g. protein folding), since $\lambda_i = e^{-k_i\,\Delta t}$ relates to each relaxation rate $k_i$.  
- In a reversible (detailed-balance) system, $\varphi_i(x) = \mu(x)\,\psi_i(x)$. In summary, the leading eigenfunction of $T_\Omega$ is the equilibrium distribution $\mu(x)$, while subsequent eigenfunctions encode slow dynamical modes.

---

## Implicit Transfer Operator Learning

**Learning Long-Timestep Dynamics.**  
The ITO framework [1] directly learns a surrogate model for the long-lag transition density $p_{N\tau}\bigl(x_{t+N\tau}\mid x_t\bigr)$, where $N$ can be large (e.g. thousands of MD steps). Equivalently, it learns the “kernel” of $T_\Omega(N\tau)$ acting on a Dirac delta:

$$
\bigl[T_\Omega(N\tau)\circ \delta_{x_t}\bigr](x_{t+N\tau})
  \;=\; p_{N\tau}\bigl(x_{t+N\tau}\mid x_t\bigr).
  \tag{4}
$$

By sampling from this learned conditional density, one can “jump” over $N$ steps in a single shot, greatly accelerating long-timescale sampling.

**Score-Based Generative Models.**  
ITO uses a **score-based** (diffusion) generative model [3,4] to represent $p_{N\tau}\bigl(x_{t+N\tau}\mid x_t\bigr)$. The workflow is:

1. **Forward noising SDE:**  
   Given data sample $x^0\in \mathbb{R}^d$ (a molecular configuration), define a fixed SDE that gradually adds noise:

   $$
   dx^t \;=\; f\bigl(x^t, t\bigr)\,dt \;+\; g(t)\,dW_t, \quad t\in [0, T],
   $$

   where $x^{0} = x$ is the clean sample and $x^{T}$ is approximately standard Gaussian noise.  

2. **Score network training:**  
   Train a neural network $s_\theta\bigl(x^t,\,t\bigr)$ to approximate the score function $\nabla_x \log p^t(x)$ at each diffusion time $t\in[0,T]$.  

3. **Reverse-time SDE:**  
   At inference, start from $x^{T}\sim\mathcal{N}(0,I)$ and solve the learned reverse SDE to generate samples approximating $p^0(x)$.

**Conditional Score Models for Dynamics.**  
ITO extends this to a **conditional** diffusion. Denote by $x_{t+N\tau}^{t_{\mathrm{diff}}}$ a “noised” version of the future configuration $x_{t+N\tau}$. Then train

$$
s_\theta\bigl(x_{t+N\tau}^{\,t_{\mathrm{diff}}},\,x_t,\,N,\,t_{\mathrm{diff}}\bigr)
\;\approx\; \nabla_{x_{t+N\tau}} \log p\bigl(x_{t+N\tau}^{\,t_{\mathrm{diff}}}\mid x_t\bigr).
$$

When fully denoised ($t_{\mathrm{diff}}=0$), the network directly models the target conditional density $p\bigl(x_{t+N\tau}\mid x_t\bigr)$.

**Multi-Resolution Training.**  
Rather than fixing a single lag $N$, ITO randomly samples $N$ from a set of values \{short, medium, long\} during training. Concretely:

- From MD trajectories, collect pairs $\bigl(x_t,\;x_{t+N\tau}\bigr)$ for various $N$’s.  
- Feed $(x_{t+N\tau}^{\,t_{\mathrm{diff}}},\,x_t,\,N,\,t_{\mathrm{diff}})$ into $s_\theta$.  

This encourages self-consistency across multiple time scales and yields better learning of slow dynamical modes than training on a single fixed lag.

---

## Boltzmann Priors for ITO

While ITO provides a framework for long-lag dynamics, it can require large amounts of MD data to capture rare events. BoPITO [2] addresses this by using **Boltzmann Generators (BG)** [6] in two ways:

1. **Efficient Data Collection via BG:**  
   - Instead of running a few long MD trajectories, first sample many configurations from the equilibrium Boltzmann distribution $\mu(x)$ using a pretrained BG.  
   - From each sampled configuration, run a short MD trajectory (e.g. a few hundred picoseconds) to collect transitions.  
   - This “many short simulations” strategy covers diverse metastable basins more efficiently, reducing total MD cost by an order of magnitude for training ITO.

2. **Incorporating Equilibrium Knowledge into the Score Model:**  
   Transfer operator theory tells us the first eigenfunction is $\mu(x)$ and others decay as $|\lambda_i|<1$. BoPITO explicitly decomposes the conditional score:

   $$
   \begin{aligned}
   s_\theta\bigl(x_{t+N\tau}^{\,t_{\mathrm{diff}}},\,x_t,\,N,\,t_{\mathrm{diff}}\bigr)
   &=\,s_{\mathrm{eq}}\bigl(x_{t+N\tau}^{\,t_{\mathrm{diff}}},\,t_{\mathrm{diff}}\bigr)
     \;+\;\hat{\lambda}^N\,
     s_{\mathrm{dyn}}\bigl(x_{t+N\tau}^{\,t_{\mathrm{diff}}},\,x_t,\,N,\,t_{\mathrm{diff}}\bigr),
     \tag{6}
   \end{aligned}
   $$

   where  
   - $s_{\mathrm{eq}}(x,\,t_{\mathrm{diff}}) = \nabla_x \log \mu(x)$ is the known equilibrium score (obtainable from BG or directly from $-\beta\nabla U(x)$), and  
   - $s_{\mathrm{dyn}}(\cdot)$ models the remaining dynamical modes, scaled by $\hat{\lambda}^N$ (with $0<\hat{\lambda}<1$ a hyperparameter).  

   Because $\lambda_1=1$, in the limit $N\to\infty$ we have $\hat{\lambda}^N \to 0$ and $s_\theta \approx s_{\mathrm{eq}}$, ensuring the learned model recovers $\mu(x)$ at large time lags. This spectral-inspired decomposition sharply reduces the learning burden on equilibrium density, focusing data on the slowly decaying modes only when necessary.

![Figure 2: BoPITO leverages pre-trained Boltzmann Generators for data-efficient ITO training.](https://hackmd.io/_uploads/Hk19rHgGxg.png)  
###### *Figure 2. BoPITO leverages pretrained Boltzmann Generators to enable data-efficient training of ITO models of the transition density. (출처: [2])*

---

## Conclusion

**Implicit Transfer Operator Learning** introduced score-based diffusion models to learn multi‐time‐step surrogates for MD, enabling long‐timescale sampling far faster than brute‐force simulations when provided enough training data. **Boltzmann Priors for ITO** further improves data efficiency and equilibrium accuracy by (1) using Boltzmann Generators to gather diversified short‐trajectory data and (2) decomposing the score function into known equilibrium and learned dynamical parts, ensuring correct Boltzmann behavior at long lags. Together, these advances pave the way for tackling previously intractable problems in protein folding, drug binding kinetics, and materials aging.

---

## References

1. Schreiner, Mathias; Winther, Ole; & Olsson, Simon.  
   “Implicit transfer operator learning: Multiple time-resolution models for molecular dynamics.”  
   *Advances in Neural Information Processing Systems* (NeurIPS 2023).

2. Díez, Juan Viguera; et al.  
   “Boltzmann priors for Implicit Transfer Operators.”  
   *arXiv* (2024).

3. Klein, Luke; Foong, Alexander; Fjelde, Techu; Mlodozeniec, Bruno; Brockschmidt, Marc; Nowozin, Sebastian; & Tomioka, Ryo.  
   “Timewarp: Transferable acceleration of molecular dynamics by learning time-coarsened dynamics.”  
   *Advances in Neural Information Processing Systems* 2023.

4. Ho, Jonathan; Jain, Ajay; & Abbeel, Pieter.  
   “Denoising diffusion probabilistic models.”  
   *Advances in Neural Information Processing Systems* 2020.

5. Song, Yang; et al.  
   “Score-Based Generative Modeling through Stochastic Differential Equations.”  
   *ICLR* 2020.

6. Noé, Frank; et al.  
   “Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning.”  
   *Science* 2019.
